{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleChatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJoFvqxN8KRa"
      },
      "source": [
        "Synonyms, antonyms etc for words . Using wordnet we will find all synonyms of cookbook dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeJvosLp6GRM"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Na5FoEH6ZZP",
        "outputId": "b7ffe155-f1f5-4a66-c5b9-3a5416216fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_k8N4_p6igE",
        "outputId": "ebfa51ee-1f34-452e-8cf2-4039c9c27d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E7sS3d-6vhp",
        "outputId": "355144f7-1308-437e-9ce6-cfa3c3a1d2bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "syn =wordnet.synsets('cookbook')[0]\n",
        "syn.name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'cookbook.n.01'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNXe_EmW7ib2",
        "outputId": "d47723e3-6135-4037-b4cd-4cbca1447672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "syn.definition()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'a book of recipes and cooking directions'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkqQSxHs7l_Q",
        "outputId": "e2d642f7-1baa-4215-f970-12c29238c0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "syn.examples()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex-652EE8fSc"
      },
      "source": [
        "Here we will get all the synonyms related to cooking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG03Se347sK1",
        "outputId": "d4bd7684-cbba-43d3-ec4f-dab661bb5297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "wordnet.synsets('cooking')[0].examples()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cooking can be a great art',\n",
              " 'people are needed who have experience in cookery',\n",
              " 'he left the preparation of meals to his wife']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNPrq5AY9AGc"
      },
      "source": [
        "Concept of lemmas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1ici5uk76u1"
      },
      "source": [
        "lemmas= syn.lemmas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG7I1Kod9IlW",
        "outputId": "23da9eda-600e-4f94-b167-2f02bbcc8028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lemmas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('cookbook.n.01.cookbook'), Lemma('cookbook.n.01.cookery_book')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMeTyDi9MXj",
        "outputId": "a5f63b42-8658-44ae-aa6d-9c5904937726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "lemmas[1].name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'cookery_book'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um9M68-p-KG5"
      },
      "source": [
        "Chatbot related"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU1CoVB-9aN3",
        "outputId": "9859f038-d131-4c26-ddff-7c466f4f8cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import random\n",
        "b = [\"Hello\", \"Hai\", \"Welcome\", \"Greetings\"]\n",
        "a= input(\"Message: \")\n",
        "if a== \"Hi\":\n",
        "  print(random.choice(b))\n",
        "else:\n",
        "  print(\"say it again\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Message: hi\n",
            "say it again\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jOCSZyI_WJB",
        "outputId": "66047828-1a92-4628-af14-8c8183be1d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import random\n",
        "b = [\"Hello\", \"Hai\", \"Welcome\", \"Greetings\"]\n",
        "a= input(\"Message: \")\n",
        "if a== \"Hi\":\n",
        "  print(random.choice(b))\n",
        "else:\n",
        "  print(\"say it again\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Message: Hi\n",
            "Hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_zBYWkG__n-"
      },
      "source": [
        "Steps involved in ChatBot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s57oH08j_rw-"
      },
      "source": [
        "#scikit library\n",
        "#using of nltk\n",
        "#Text preprocessing\n",
        "#uppercase to lowercase\n",
        "#tokenization\n",
        "#stopwords\n",
        "#stemming\n",
        "#lemmatization\n",
        "#TF-IDF\n",
        "#TF = (No. of times term t appear in a document)/(no. of terms in a document)\n",
        "#IDF = 1+log(N/n), where N is no. of documents and n is no. of documents a term t has appeared\n",
        "#cosine similarity(d1,d2)= Dot Product(d1,d2)/ ||d1|| * ||d2||\n",
        "# d1 and d2 must be non zero vectors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuhWw4NgCNCn"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string #to process standard python strings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdwxywvsDTMW",
        "outputId": "4c31be44-e790-41e6-c3f0-883708eb6645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "#reading text file and converting into lower cases\n",
        "f= open ('/root/nltk_data/mychat.txt','r',errors='ignore')\n",
        "raw= f.read()\n",
        "raw=raw.lower()\n",
        "raw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e0342d262856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#reading text file and converting into lower cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/root/nltk_data/mychat.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/nltk_data/mychat.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvPjcr6sDy3r"
      },
      "source": [
        "#sentence tokenizers\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(raw)\n",
        "word_tokens = nltk.word_tokenize(raw)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1sENM9IEPum",
        "outputId": "5132691d-0b6f-46b0-8c22-757c68a91be9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "#print first 2 sentences \n",
        "sent_tokens[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chatbot\\nfrom wikipedia, the free encyclopedia\\njump to navigation\\njump to search\\nfor other uses, see chatbot (disambiguation).',\n",
              " 'a virtual assistant chatbot\\neliza conversation.jpg\\n\\na chatbot is a software application used to conduct an on-line chat conversation via text or text-to-speech, in lieu of providing direct contact with a live human agent.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHllGLhOEdDJ",
        "outputId": "2cac80b6-fb3d-4ccb-a221-abe841267807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#print first 2 words\n",
        "word_tokens[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chatbot', 'from']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpFmuolJEmzT"
      },
      "source": [
        "#stemming\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cpIzrmZGVRH"
      },
      "source": [
        "GREETING_INPUTS = (\"hello\",\"hi\",\"greetings\",\"hey\",\"welcome\")\n",
        "GREETING_RESPONSES = [\"hi\",\"hey\",\"hi there\", \"hello\"]\n",
        "def greeting(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREETING_INPUTS:\n",
        "      return random.choice(GREETING_RESPONSES)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1otTZmvHVCi"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZInlxlgHkua"
      },
      "source": [
        "def response(user_response):\n",
        "  robo_response = ''\n",
        "  sent_tokens.append(user_response)\n",
        "\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf) #checking the last index value\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flattern()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "\n",
        "  if(req_tfidf==0):\n",
        "    robo_response = robo_response+\"I am sorry! I dont understand you\"\n",
        "    return robo_response\n",
        "  else:\n",
        "    robo_response = robo_response + sent_tokens[idx]\n",
        "    return robo_response\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znhuz2fRJeDf",
        "outputId": "5d4c3bc7-7c79-4177-a936-c2f10c173d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "#Activate robot\n",
        "\n",
        "flag = True\n",
        "print(\"ROBO: My name is chitti. I will answer your queries. If\\\n",
        "\")\n",
        "\n",
        "while(flag == True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if(user_response!= 'bye'):\n",
        "    if(user_response == 'thanks' or user_response == 'thank you'):\n",
        "      flag = False\n",
        "      print (\"ROBO : You are Welcome..\")\n",
        "    else:\n",
        "      if(greeting(user_response) != None):\n",
        "        print(\"ROBO: \"+greeting(user_response))\n",
        "      else:\n",
        "        print(\"ROBO:\", end =\"\")\n",
        "        print(response(user_response))\n",
        "        sent_tokens.remove(user_response)\n",
        "  else:\n",
        "      flag = False\n",
        "      print(\"ROBO: Bye take care\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROBO: My name is chitti. I will answer your queries. If\n",
            "hi\n",
            "ROBO: hi there\n",
            "dhoni\n",
            "ROBO:"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-84bebd37e40b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ROBO:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0msent_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-78219780694c>\u001b[0m in \u001b[0;36mresponse\u001b[0;34m(user_response)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#checking the last index value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mflat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mreq_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'flattern'"
          ]
        }
      ]
    }
  ]
}